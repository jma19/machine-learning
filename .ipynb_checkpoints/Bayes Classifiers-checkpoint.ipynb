{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and probability\n",
    "\n",
    "1.  Prior probability of each class----------------p(y) \n",
    "2.  Distribution of features given the class------p(x|y=c)\n",
    "3.  Joint distribution\n",
    "$$p(y|x)*p(x) = p(x,y)=p(x|y)*p(y)$$\n",
    "\n",
    "4. BayesRule\n",
    "$$p(y|x) = p(x|y) * p(y)/p(x) = \\frac{p(x|y)* p(y)}{\\sum_{c} p(x|y=c)*p(y=c)}$$\n",
    "\n",
    "Samples:\n",
    "\n",
    "![](samples.png)\n",
    "\n",
    "### Gaussian models\n",
    "We can Estimate a probability model for each class. As for Gaussian Model, expected value μ and standard deviation σ: \n",
    "$$μ = \\frac{1}{m} \\sum_{j} x^j$$\n",
    "$$σ^2 = \\frac{1}{m} \\sum_{j} (x^j - u)^2 = \\sum_{j} (x^j - u)^T(x^j - u)$$\n",
    "\n",
    "The gaussian distribution can be:\n",
    "\n",
    "$$N(x;μ,σ) = \\frac{1}{\\sqrt{2pi* σ^2}}e^{\\frac{(x-μ)^2}{2σ^2}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifiers \n",
    "1. Estimatep(y)=[p(y=0),p(y=1)...]\n",
    "2. Estimate p(x|y=c) for each class c\n",
    "3. Calculate p(y=c|x) using Bayes rule\n",
    "4. Choose the most likely class\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Overfitting and Its solution\n",
    "\n",
    "Overfitting occurs when a model is excessively complex, such as having too many parameters relative to the number of observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use a joint distribution to make predications ?\n",
    "\n",
    "We can use the joint distribution to predict sample's classfication, only on the condition that training data have all \n",
    "degrees of freedom(all combination conditions of independent feature). However, it is usually impossible. \n",
    "\n",
    "Using joint bayers can't handle the situation of new fetaure combination, for example, as for x=(0 0 0 0 0), we can't use conditional joint probility to predict the class, because there is no feature combination (0 0 0 0 0) in training data. However, through using naive bayes, we can transform the computation from joint probility to independent probility (it looks like: p((x1,x2,x3)|y) = p(x1|y) p (x2|y) p(x3|y) for x1, x2, x3 are independent), which make us more easier to predict new samples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
